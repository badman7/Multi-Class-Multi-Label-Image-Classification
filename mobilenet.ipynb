{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnO_6mefLrPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff7a7a6e-0baf-4815-de0d-6f3acc86a86e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irssfx-BgXXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwE7UesmLuTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ec27dc9-3fc3-42c2-9759-e0609865ceec"
      },
      "source": [
        "cd /content/drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BateqxGML1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from models import Multimobilenet,Multialexnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rX9UiWbMQOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Dataset import train_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMrpAjO6MSNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Hnxra7MUFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpwVLYkWMY2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('Data_Entry_2017.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrxe3Ne7Mbtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images=df['Image Index']\n",
        "labels=df['Finding Labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF9SQaNvMdr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images=[]\n",
        "test_labels=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqpJVTPIMfVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images=images[:4000]\n",
        "train_labels=labels[:4000]\n",
        "test_images[0:]=images[4000:]\n",
        "test_labels[0:]=labels[4000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zasKLtUvMg5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images,train_labels=get_existing_images('/content/drive/My Drive/images_001/images',train_images,train_labels)\n",
        "test_images,test_labels=get_existing_images('/content/drive/My Drive/images_001/images',test_images,test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_BYR-U6NJhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f466216f-d983-4b35-9733-0b1855755f3e"
      },
      "source": [
        "encoder=encode_labels(labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ydXLrU17q4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16ce900a-8ba7-4502-f17a-25dd23b5d85c"
      },
      "source": [
        "decoder={}\n",
        "for keys in encoder.keys():\n",
        "  decoder[encoder[keys]]=keys\n",
        "decoder[3]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No Finding'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvoFand4O75z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform=image_transform(250,225)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YNHcmvhPA0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import transforms,datasets\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import sampler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFh_6uJFvX9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size=100\n",
        "test_batch_size=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHYshAQBPEzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingset = train_dataset(Labels=train_labels,images=train_images ,encoder=encoder,dataset='/content/drive/My Drive/images_001/images',transform=transform)\n",
        "trainloader = DataLoader(trainingset,batch_size = train_batch_size,shuffle = True)\n",
        "testset = train_dataset(Labels=test_labels,images=test_images ,encoder=encoder,dataset='/content/drive/My Drive/images_001/images',transform=transform)\n",
        "testloader = DataLoader(testset,batch_size = test_batch_size,shuffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SJYjuRTRk5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "407d30ec-d678-43f3-9a3a-0b68e6ec5288"
      },
      "source": [
        "!pip install tensorboardX --no-cache-dir"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 24.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 27.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 28.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 29.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 25.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 27.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 28.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 28.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 28.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 28.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 28.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 28.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 28.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 28.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 28.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 28.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LijWbwl_SP84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVkEO9duSTxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_epochs = 10\n",
        "model = Multimobilenet()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2so_BBdZSYhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss=nn.BCEWithLogitsLoss(reduction='mean') # loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyyJKWx7A2eB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "def get_most_recent_checkpoint(checkpoint_dir):\n",
        "    checkpoint_paths = [path for path in glob(\"{}/model_epoch_*.pth\".format(checkpoint_dir))]\n",
        "    idxes = [int(os.path.basename(path).split('_')[2].split('.')[0]) for path in checkpoint_paths]\n",
        "\n",
        "    max_idx = max(idxes)\n",
        "    latest_checkpoint = os.path.join(checkpoint_dir, \"model_epoch_{}.pth\".format(max_idx))\n",
        "    print(\" [*] Found latest checkpoint: {}\".format(latest_checkpoint))\n",
        "    return latest_checkpoint, max_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0J4UKJzA5Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def checkpoint(epoch):\n",
        "  model_out_path = \"/content/drive/My Drive/ckpt/\" + \"mobilenet\" + \"/model_epoch_{}.pth\".format(epoch)\n",
        "  '''\n",
        "    Model Implementation\n",
        "    elif opt.model == \"Model_Name\":\n",
        "        model_out_path = \"ckpt/\" + \"Model_Name\" + \"/model_epoch_{}.pth\".format(epoch)\n",
        "  '''\n",
        "  print(model_out_path)\n",
        "  torch.save({\n",
        "            'model': model.state_dict(),\n",
        "            }, model_out_path)\n",
        "  print(\"Checkpoint saved to {}\".format(model_out_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r018O9-CSfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if len(next(os.walk('/content/drive/My Drive/ckpt/mobilenet'))[2]) != 0:\n",
        "  \n",
        "  min_iter = 0\n",
        "  last_ckpt, min_iter = get_most_recent_checkpoint('/content/drive/My Drive/ckpt/mobilenet')\n",
        "  min_iter=min_iter+1\n",
        "  print('Getting most recent checkpoint -------->')\n",
        "  checkpoint=torch.load(last_ckpt)\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "else:\n",
        "  min_iter=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-FKr8YnSbJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT1cKuBWSqQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16vpeGref6u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat models.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GjjFpwXgFdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2adb883e-9b8c-4a62-9d09-839c2b394b52"
      },
      "source": [
        "%%writefile models.py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class Multimobilenet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
        "        last_channel = models.mobilenet_v2().last_channel # size of the layer before the classifier\n",
        "\n",
        "        # the input for the classifier should be two-dimensional, but we will have\n",
        "        # [<batch_size&gt;, <channels&gt;, <width&gt;, <height&gt;]\n",
        "        # so, let's do the spatial averaging: reduce <width&gt; and <height&gt; to 1\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # create separate classifiers for our outputs\n",
        "        self.Type = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=last_channel, out_features=2)\n",
        "        )\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=last_channel, out_features=15)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.pool(x)\n",
        "        \n",
        "      # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        \n",
        "        return {\n",
        "          'Type':F.softmax(self.Type(x)),\n",
        "          'targets':F.softmax(self.target(x))\n",
        "         }\n",
        "\n",
        "\n",
        "\n",
        "class Multialexnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.base_model = models.alexnet()  # take the model without classifier\n",
        "                # the input for the classifier should be two-dimensional, but we will have\n",
        "        # [<batch_size&gt;, <channels&gt;, <width&gt;, <height&gt;]\n",
        "        # so, let's do the spatial averaging: reduce <width&gt; and <height&gt; to 1\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # create separate classifiers for our outputs\n",
        "        self.Type = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=1000, out_features=2)\n",
        "        )\n",
        "        self.target = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=1000, out_features=15)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        #print('Passing 1st phase')\n",
        "        x = self.base_model(x)\n",
        "        #print('Passing 2nd phase')\n",
        "        #x = self.pool(x)\n",
        "\n",
        "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
        "        #print('Flattening')\n",
        "        #x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        return {\n",
        "          'Type': F.softmax(self.Type(x)),\n",
        "          'targets': F.softmax(self.target(x))\n",
        "        }"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbFnKb6RvJi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_evaluator(pred_labels, true_labels):\n",
        "    pred_labels = torch.ByteTensor(pred_labels)\n",
        "    true_labels = torch.ByteTensor(true_labels)\n",
        "\n",
        "    assert 1 >= pred_labels.all() >= 0\n",
        "    assert 1 >= true_labels.all() >= 0\n",
        "\n",
        "    # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
        "    TP = torch.sum((pred_labels >= 0.7) & ((true_labels == 1)))\n",
        "\n",
        "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
        "    TN = torch.sum((pred_labels < 0.7) & (true_labels == 0))\n",
        "\n",
        "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
        "    FP = torch.sum((pred_labels >= 0.7) & (true_labels == 0))\n",
        "\n",
        "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
        "    FN = torch.sum((pred_labels < 0.7) & (true_labels == 1))\n",
        "    return TP+TN-FP-FN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDKsOegNg3FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch=next(iter(testloader))\n",
        "img=batch['img']\n",
        "img=img.to(device)\n",
        "target_labels=batch['labels']\n",
        "output=model(img)\n",
        "loss_type=loss(output['Type'].to(device),target_labels['Type'].to(device))\n",
        "loss_target=loss(output['targets'].to(device),target_labels['targets'].to(device))\n",
        "testloss_type= loss_type.item()\n",
        "testloss_target=loss_target.item()\n",
        "accuracy_for_type=accuracy_evaluator(output['Type'],target_labels['Type'])/test_batch_size\n",
        "accuracy_for_target=accuracy_evaluator(output['targets'],target_labels['targets'])/test_batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vH8DBvhthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGfi_H15hQhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BkLIb8BSsuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "0fa5c5d5-fb9e-4c4c-9804-ac08e71cf5a6"
      },
      "source": [
        "for epoch in range(min_iter, N_epochs + 1):\n",
        "  total_loss=0\n",
        "  for k,batch in enumerate(trainloader):\n",
        "    optimizer.zero_grad()\n",
        "    img = batch['img']\n",
        "    img=img.to(device)\n",
        "    target_labels = batch['labels']\n",
        "    output = model(img)\n",
        "    loss_type=loss(output['Type'].to(device),target_labels['Type'].to(device))\n",
        "    loss_target=loss(output['targets'].to(device),target_labels['targets'].to(device))\n",
        "    curr_loss= loss_type.item()+loss_target.item()\n",
        "    total_loss+=curr_loss\n",
        "    loss_type.backward(retain_graph=True)\n",
        "    loss_target.backward()\n",
        "    optimizer.step()\n",
        "    writer.add_scalar('Multi Class Loss',loss_type.item())\n",
        "    writer.add_scalar('Multi Label Loss',loss_target.item())\n",
        "    print('completed batch',k+1,' Total number of batches : ',len(trainloader),' Class Loss = ',loss_type.item(),'Label Loss = ',loss_target.item())\n",
        "  print('Done for epoch ',epoch+1)\n",
        "  print('loss for epoch = ',total_loss/len(trainloader))\n",
        "  checkpoint(epoch)\n",
        "  testloss_type=0\n",
        "  testloss_target=0\n",
        "  accuracy_for_type=0\n",
        "  accuracy_for_target=0\n",
        "  for k,batch in enumerate(testloader):\n",
        "    img=batch['img']\n",
        "    img=img.to(device)\n",
        "    target_labels=batch['labels']\n",
        "    output=model(img)\n",
        "    loss_type=loss(output['Type'].to(device),target_labels['Type'].to(device))\n",
        "    loss_target=loss(output['targets'].to(device),target_labels['targets'].to(device))\n",
        "    testloss_type+= loss_type.item()\n",
        "    testloss_target+=loss_target.item()\n",
        "    accuracy_for_type+=accuracy_evaluator(output['Type'],target_labels['Type'])/test_batch_size\n",
        "    accuracy_for_target+=accuracy_evaluator(output['targets'],target_labels['targets'])/test_batch_size\n",
        "  print('Accuracy for Normal/Abnormal Class : ',accuracy_for_type/(len(testloader)))\n",
        "  print('Accuracy for Labels : ',accuracy_for_target/len(testloader))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "completed batch 1  Total number of batches :  40  Class Loss =  0.6934219002723694 Label Loss =  0.6933416128158569\n",
            "completed batch 2  Total number of batches :  40  Class Loss =  0.6925032138824463 Label Loss =  0.6929797530174255\n",
            "completed batch 3  Total number of batches :  40  Class Loss =  0.6869975328445435 Label Loss =  0.6932076811790466\n",
            "completed batch 4  Total number of batches :  40  Class Loss =  0.6974266171455383 Label Loss =  0.693162739276886\n",
            "completed batch 5  Total number of batches :  40  Class Loss =  0.6931918263435364 Label Loss =  0.6931448578834534\n",
            "completed batch 6  Total number of batches :  40  Class Loss =  0.6931365728378296 Label Loss =  0.6931506991386414\n",
            "completed batch 7  Total number of batches :  40  Class Loss =  0.6931406855583191 Label Loss =  0.6931411027908325\n",
            "completed batch 8  Total number of batches :  40  Class Loss =  0.6931484341621399 Label Loss =  0.693150520324707\n",
            "completed batch 9  Total number of batches :  40  Class Loss =  0.6931431293487549 Label Loss =  0.6931455135345459\n",
            "completed batch 10  Total number of batches :  40  Class Loss =  0.6931517720222473 Label Loss =  0.693149745464325\n",
            "completed batch 11  Total number of batches :  40  Class Loss =  0.6931465268135071 Label Loss =  0.6931344270706177\n",
            "completed batch 12  Total number of batches :  40  Class Loss =  0.6931487917900085 Label Loss =  0.6931487321853638\n",
            "completed batch 13  Total number of batches :  40  Class Loss =  0.6931509375572205 Label Loss =  0.693141758441925\n",
            "completed batch 14  Total number of batches :  40  Class Loss =  0.6931481957435608 Label Loss =  0.6931535005569458\n",
            "completed batch 15  Total number of batches :  40  Class Loss =  0.6931490898132324 Label Loss =  0.6931469440460205\n",
            "completed batch 16  Total number of batches :  40  Class Loss =  0.6931472420692444 Label Loss =  0.6931469440460205\n",
            "completed batch 17  Total number of batches :  40  Class Loss =  0.6931464076042175 Label Loss =  0.693146824836731\n",
            "completed batch 18  Total number of batches :  40  Class Loss =  0.6931474804878235 Label Loss =  0.6931473016738892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-226d499883b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtotal_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimage_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#plt.imshow(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQn3MvAJS4UI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs0j4DWYPL1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkKyYJrCMjn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}